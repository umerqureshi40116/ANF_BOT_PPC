query_data.py:

################################ QUERY_DATA.PY ################################
from dotenv import load_dotenv
import os
from pinecone import Pinecone
import re

load_dotenv()
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
INDEX_NAME = "anf-ppc-4"

# Connect to Pinecone
pc = Pinecone(api_key=PINECONE_API_KEY)
index = pc.Index(INDEX_NAME)

def query_section(section_number: str, top_k: int = 5):
    print(f"\nðŸ” Searching for Section {section_number}...\n")

    # Dummy vector since we only care about filter
    dummy_vector = [0.1] * 768  

    response = index.query(
        vector=dummy_vector,
        top_k=top_k,
        include_metadata=True,
        filter={"section": section_number}
    )

    if not response.matches:
        print(f"âš ï¸ No matches found for Section {section_number}.")
        return

    for i, match in enumerate(response.matches, 1):
        meta = match.metadata
        source = meta.get("source", "Unknown")
        section = meta.get("section", "N/A")
        title = meta.get("title", "N/A")
        text = meta.get("text", "âš ï¸ No text available")

        print(f"\nðŸ“Œ Result {i}")
        print(f"Source: {source} | Section: {section} | Title: {title}")
        print("-" * 80)
        print(text[:1200])  # first 1200 chars
        print("-" * 80)

if __name__ == "__main__":
    query_section("218")




create_embeddings.py:

################################ CREATE_DATABASE.PY (FIXED) ################################
import os
import re
from sentence_transformers import SentenceTransformer
from pinecone import Pinecone, ServerlessSpec
from dotenv import load_dotenv
import pdfplumber
from pdf2image import convert_from_path
import pytesseract
from tqdm import tqdm

# ==============================
# 1. Load environment variables
# ==============================
load_dotenv()
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
INDEX_NAME = "anf-ppc-4"

pytesseract.pytesseract.tesseract_cmd = r"D:\UMER_ANF\tesseract\tesseract.exe"
DATA_DIR = r"D:\UMER_ANF\ANF_Educational_Chatbot_Working_Backup-main\data\books"
POPPLER_PATH = r"D:\UMER_ANF\poppler-25.07.0\Library\bin"

# ==============================
# 2. PDF Extraction
# ==============================
def extract_text_from_pdf(pdf_path):
    text = ""
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
    except Exception as e:
        print(f"âš ï¸ pdfplumber failed on {pdf_path}: {e}")

    if not text.strip():
        pages = convert_from_path(pdf_path, dpi=300, poppler_path=POPPLER_PATH)
        for page in pages:
            text += pytesseract.image_to_string(page, lang="eng") + "\n"

    return text

def clean_text(text):
    # Fix spacing issues
    text = re.sub(r'(\w)\s+(\w)(?=\s+\w|\s*[.,;:])', r'\1\2', text)
    text = re.sub(r'\n\s*\n', '\n\n', text)
    text = re.sub(r' +', ' ', text)
    return text.strip()

# ==============================
# 3. Section Extraction (Robust)
# ==============================
def extract_sections(text):
    """
    Extract sections robustly for PPC.
    """
    # More robust pattern that handles various section formats
    pattern = re.compile(
        r'^\s*(\d+[A-Z]*)\.\s+(.*?)(?=\s*^\s*\d+[A-Z]*\.\s|\Z)',
        re.MULTILINE | re.DOTALL
    )
    
    sections = []
    for match in pattern.finditer(text):
        section_num = match.group(1).strip()
        section_text = match.group(2).strip()
        
        if not section_text:
            continue

        # Extract title (first line or first 100 chars)
        lines = section_text.split('\n')
        title_line = lines[0].strip() if lines else ""
        if len(title_line) > 100:
            title_line = title_line[:100] + "..."
            
        # Clean up the text
        section_text = re.sub(r'\n\s*\n', '\n\n', section_text)
        section_text = re.sub(r' +', ' ', section_text)
        
        sections.append({
            "section": section_num,
            "title": title_line,
            "text": section_text
        })
        
        # Debug: print section 171 when found
        if section_num == "171":
            print(f"âœ“ Found Section 171: {title_line}")
            print(f"Text preview: {section_text[:200]}...")

    return sections

# ==============================
# 4. Section-aware chunking
# ==============================
def chunk_section_safe(section, max_words=1200):
    words = section["text"].split()
    if len(words) <= max_words:
        return [section]

    chunks = []
    for i in range(0, len(words), max_words):
        chunk_text = " ".join(words[i:i + max_words])
        chunks.append({
            "section": section["section"],
            "title": section["title"],
            "text": chunk_text
        })
    return chunks

# ==============================
# 5. Build Pinecone Index
# ==============================
def build_index(data_dir):
    print("Loading embeddings model (768-d)...")
    model = SentenceTransformer("sentence-transformers/all-mpnet-base-v2")

    print("Connecting to Pinecone...")
    pc = Pinecone(api_key=PINECONE_API_KEY)

    if INDEX_NAME not in pc.list_indexes().names():
        print(f"Creating Pinecone index '{INDEX_NAME}'...")
        pc.create_index(
            name=INDEX_NAME,
            dimension=768,
            metric="cosine",
            spec=ServerlessSpec(cloud="aws", region="us-east-1")
        )
    index = pc.Index(INDEX_NAME)

    all_vectors = []

    for file in os.listdir(data_dir):
        if file.lower().endswith(".pdf"):
            pdf_path = os.path.join(data_dir, file)
            print(f"\nðŸ“– Processing {pdf_path}")
            text = clean_text(extract_text_from_pdf(pdf_path))
            if not text:
                print(f"âš ï¸ No text found in {file}")
                continue

            sections = extract_sections(text)
            print(f"  â†’ {len(sections)} sections extracted")
            if not sections:
                print(f"âš ï¸ No sections matched in {file}, skipping...")
                continue

            # Chunk sections
            all_section_chunks = []
            for sec in sections:
                sec_chunks = chunk_section_safe(sec)
                all_section_chunks.extend(sec_chunks)

            texts = [s["text"] for s in all_section_chunks]
            embeddings = model.encode(texts, batch_size=32, show_progress_bar=True)

            # Prepare vectors
            for i, s in enumerate(all_section_chunks):
                vec_id = f"{file}_section_{s['section']}_{i}"
                all_vectors.append(
                    (
                        vec_id,
                        embeddings[i].tolist(),
                        {
                            "source": file,
                            "section": s["section"],
                            "title": s["title"],
                            "text": s["text"]
                        }
                    )
                )

    # Upsert in batches
    if all_vectors:
        print(f"\nUpserting {len(all_vectors)} vectors to Pinecone in batches...")
        BATCH_SIZE = 100
        for i in tqdm(range(0, len(all_vectors), BATCH_SIZE), desc="Upserting"):
            batch = all_vectors[i:i+BATCH_SIZE]
            index.upsert(vectors=batch)
        print("âœ“ Database build complete!")
    else:
        print("âš ï¸ No vectors to insert.")

# ==============================
# 6. Run
# ==============================
if __name__ == "__main__":
    build_index(DATA_DIR)
